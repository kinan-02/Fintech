{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kinan-02/Fintech/blob/main/LSTM_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "# Load the Parquet file\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(0)\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "df = pd.read_parquet('final_data.parquet')\n",
        "df_test = pd.read_parquet('test_data.parquet')\n",
        "\n",
        "\n",
        "features = df[['open_btc', 'high_btc', 'low_btc', 'trade_count_btc',\n",
        "               'open_eth', 'high_eth', 'low_eth', 'close_eth', 'trade_count_eth',\n",
        "               'open_cl', 'close_cl', 'high_cl', 'low_cl', 'trade_count_cl',\n",
        "               'open_gold', 'close_gold', 'high_gold', 'low_gold', 'trade_count_gold',\n",
        "               'open_uup', 'close_uup', 'high_uup', 'low_uup', 'trade_count_uup',\n",
        "               'open_mstr', 'close_mstr', 'high_mstr', 'low_mstr', 'trade_count_mstr',\n",
        "               'open_spy', 'close_spy', 'high_spy', 'low_spy', 'trade_count_spy',\n",
        "               'open_ndaq', 'close_ndaq', 'high_ndaq', 'low_ndaq', 'trade_count_ndaq']]\n",
        "\n",
        "test_features = df_test[['open_btc', 'high_btc', 'low_btc', 'trade_count_btc',\n",
        "               'open_eth', 'high_eth', 'low_eth', 'close_eth', 'trade_count_eth',\n",
        "               'open_cl', 'close_cl', 'high_cl', 'low_cl', 'trade_count_cl',\n",
        "               'open_gold', 'close_gold', 'high_gold', 'low_gold', 'trade_count_gold',\n",
        "               'open_uup', 'close_uup', 'high_uup', 'low_uup', 'trade_count_uup',\n",
        "               'open_mstr', 'close_mstr', 'high_mstr', 'low_mstr', 'trade_count_mstr',\n",
        "               'open_spy', 'close_spy', 'high_spy', 'low_spy', 'trade_count_spy',\n",
        "               'open_ndaq', 'close_ndaq', 'high_ndaq', 'low_ndaq', 'trade_count_ndaq']]\n",
        "\n",
        "\n",
        "target = df['close_btc'].shift(-1)\n",
        "target_test = df_test['close_btc']\n",
        "\n",
        "features = features[:-1]\n",
        "target = target[:-1]\n",
        "test_features = test_features[:-1]\n",
        "target_test = target_test[:-1]\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "features = scaler.fit_transform(features)\n",
        "test_features = scaler.transform(test_features)\n",
        "\n",
        "\n",
        "X_train, X_test = features, test_features\n",
        "y_train, y_test = target, target_test\n",
        "\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "class LSTM_Transformer(nn.Module):\n",
        "    def _init_(self, n_layers, tr_layers, n_heads_first, n_heads_second, input_dim, hidden_dim, output_dim, dropout):\n",
        "        super()._init_()\n",
        "        self.n_layers = n_layers\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout = dropout\n",
        "        self.tr_layers = tr_layers\n",
        "        self.n_heads_first = n_heads_first\n",
        "        self.n_heads_second = n_heads_second\n",
        "        self.input_fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim * 2),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim * 2, hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "\n",
        "        self.main_task = nn.LSTM(input_size=hidden_dim,\n",
        "                                 hidden_size=hidden_dim,\n",
        "                                 batch_first=True,\n",
        "                                 num_layers=n_layers,\n",
        "                                 dropout=dropout)\n",
        "\n",
        "        self.output_fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, output_dim)\n",
        "        )\n",
        "\n",
        "\n",
        "    def generate_mask(self, size):\n",
        "        \"\"\"\n",
        "        Generate a causal mask for the Transformer, preventing future positions from attending to earlier positions.\n",
        "        \"\"\"\n",
        "        mask = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, input_vec):\n",
        "\n",
        "        lstm_input = self.input_fc(input_vec)\n",
        "\n",
        "        final_output, _ = self.main_task(lstm_input)\n",
        "        output = self.output_fc(final_output)\n",
        "\n",
        "        return output.squeeze()\n",
        "\n",
        "def evaluate_model(model):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    total_loss = 0\n",
        "    total_percentage_loss = 0\n",
        "    total_mape = 0\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            output = model(X_batch)\n",
        "            loss = loss_fn(output, y_batch)\n",
        "            total_loss += loss.item()\n",
        "            percentage_loss = torch.sqrt(loss) / (torch.mean(y_batch) + 1e-10) * 100\n",
        "            total_percentage_loss += percentage_loss.item()\n",
        "            mape = torch.mean(torch.abs((y_batch - output) / (y_batch + 1e-10))) * 100\n",
        "            total_mape += mape.item()\n",
        "            predictions.append(output.numpy())\n",
        "            actuals.append(y_batch.numpy())\n",
        "    predictions[-1] = np.repeat(predictions[-1], 32)\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    actuals = np.concatenate(actuals, axis=0)\n",
        "    predicted_returns = (predictions[1:] - predictions[:-1]) / predictions[:-1]\n",
        "    real_returns = (actuals[1:] - actuals[:-1]) / actuals[:-1]\n",
        "    predicted_var = calculate_var(predicted_returns, confidence_level=90)\n",
        "    real_var = calculate_var(real_returns, confidence_level=90)\n",
        "\n",
        "    print(f\"Test Loss: {total_loss / len(test_loader):.6f}\")\n",
        "    print(f\"Test Loss as Percentage: {total_percentage_loss / len(test_loader):.2f}%\")\n",
        "    print(f\"Predicted VaR (95% confidence): {predicted_var}\")\n",
        "    print(f\"Real VaR (95% confidence): {real_var}\")\n",
        "    print(f\"Mean Absolute Percentage Error (MAPE): {total_mape / len(test_loader):.2f}%\")\n",
        "    return predictions, actuals\n",
        "\n",
        "def train_model():\n",
        "\n",
        "    model = LSTM_Transformer(\n",
        "        n_layers=1, tr_layers=1, n_heads_first=4, n_heads_second=1,\n",
        "        input_dim=X_train.shape[1], hidden_dim=32, output_dim=1, dropout=0\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch in range(15):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{30}', leave=False)\n",
        "        for X_batch, y_batch in progress_bar:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(X_batch)\n",
        "            loss = loss_fn(output, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': total_loss / len(train_loader)})\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def calculate_var(returns, confidence_level=90):\n",
        "    \"\"\"\n",
        "    Calculate Value at Risk (VaR) for a given set of returns and confidence level.\n",
        "    \"\"\"\n",
        "    return np.percentile(returns, 100 - confidence_level)\n",
        "\n",
        "trained_model = train_model()\n",
        "\n",
        "pred, actuals = evaluate_model(trained_model)"
      ],
      "metadata": {
        "id": "tcEN2qXzk3rB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c8c91cd-7f08-4c8b-ad3d-f27231e04629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 512258696.3510183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Loss: 62788.518012466695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Loss: 11108.828988024105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Loss: 9724.647349790881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Loss: 8483.221743297554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Loss: 7881.020328774298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Loss: 7416.979719793098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Loss: 7158.673284597441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Loss: 6725.663637090716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 6713.366759679007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Loss: 6409.467191107721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Loss: 6318.096300562213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Loss: 5784.610060838906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Loss: 5858.068744766244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Loss: 5644.338383316089\n",
            "Test Loss: 4162.417426\n",
            "Test Loss as Percentage: 0.09%\n",
            "Predicted VaR (95% confidence): -0.0008045249851420522\n",
            "Real VaR (95% confidence): -0.0006554247112944722\n",
            "Mean Absolute Percentage Error (MAPE): 0.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A1UWXaQ-Tu98"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}